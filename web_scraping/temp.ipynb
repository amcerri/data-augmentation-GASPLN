{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and testing the augmentation module\n",
    "\n",
    "This notebook is designed to verify the data generated for specific augmentation techniques, such as the synonym dictionary used for synonym replacement. \n",
    "\n",
    "Additionally, the notebook contains code to test actual usage of functions that will be implemented in the data augmentation module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries and packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import translators as ts\n",
    "import translators.server as tss\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading the required NLTK resources and Spacy models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Loading the synonyms into a pandas dataframe\n",
    "synonyms = pq.read_table('../data_augmentation_GASPLN/data/synonyms_pt_BR.parquet').to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 210977\n",
      "Number of words with each letter of the alphabet:\n",
      "a: 31490\n",
      "b: 5462\n",
      "c: 21891\n",
      "d: 23639\n",
      "e: 27680\n",
      "f: 7252\n",
      "g: 3422\n",
      "h: 1757\n",
      "i: 11299\n",
      "j: 830\n",
      "k: 8\n",
      "l: 4082\n",
      "m: 7933\n",
      "n: 1986\n",
      "o: 3740\n",
      "p: 16470\n",
      "q: 621\n",
      "r: 16751\n",
      "s: 10846\n",
      "t: 7240\n",
      "u: 1268\n",
      "v: 4369\n",
      "w: 3\n",
      "x: 166\n",
      "y: 0\n",
      "z: 539\n"
     ]
    }
   ],
   "source": [
    "print('Number of words: ' + str(len(synonyms)))\n",
    "print('Number of words with each letter of the alphabet:')\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyzáàâãéèêíìóòôõúùç'\n",
    "\n",
    "# Count the number of words that starts with each letter of the alphabet, store the result in a dictionary\n",
    "letter_count = {}\n",
    "\n",
    "for letter in alphabet:\n",
    "    letter_count[letter] = 0\n",
    "    \n",
    "for word in synonyms['word']:\n",
    "    # Set the word to lowercase\n",
    "    word = word.lower()\n",
    "    letter_count[word[0]] += 1\n",
    "\n",
    "# Display the number of words that starts with each letter of the alphabet\n",
    "for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    print(letter + ': ' + str(letter_count[letter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abade</td>\n",
       "      <td>[clérigo, confessor, cura, padre, prelado, pároco, sacerdote]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abadia</td>\n",
       "      <td>[convento, mosteiro, presbitério, sé, basílica, catedral, igreja, santuário, templo, ádito]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abalo</td>\n",
       "      <td>[trepar, concussão, mossa, efervescência, agitação, terremoto, emoção, comoção, choque, estremeção, trepidação, tremor, impulso, balanço, alvoroço, secussão, perturbação]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abarracamento</td>\n",
       "      <td>[acampamento, aquartelamento, bivaque]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abrigada</td>\n",
       "      <td>[resguardo, refúgio, abrigo, asilo, cobertura, reduto, valhacouto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210972</th>\n",
       "      <td>únguis</td>\n",
       "      <td>[úngue]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210973</th>\n",
       "      <td>única</td>\n",
       "      <td>[uma, inédita]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210974</th>\n",
       "      <td>único</td>\n",
       "      <td>[sempar, singular, um, uno, incomparável, ímpar, só, inédito, inconfundível, sui generis, individual]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210975</th>\n",
       "      <td>únicos</td>\n",
       "      <td>[sós, uns, individuais, incomparáveis, incomparávéis, inconfundíveis, inéditos, ímpares, singulares]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210976</th>\n",
       "      <td>úteis</td>\n",
       "      <td>[útil]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210977 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  \\\n",
       "0               Abade   \n",
       "1              Abadia   \n",
       "2               Abalo   \n",
       "3       Abarracamento   \n",
       "4            Abrigada   \n",
       "...               ...   \n",
       "210972         únguis   \n",
       "210973          única   \n",
       "210974          único   \n",
       "210975         únicos   \n",
       "210976          úteis   \n",
       "\n",
       "                                                                                                                                                                          synonyms  \n",
       "0                                                                                                                    [clérigo, confessor, cura, padre, prelado, pároco, sacerdote]  \n",
       "1                                                                                      [convento, mosteiro, presbitério, sé, basílica, catedral, igreja, santuário, templo, ádito]  \n",
       "2       [trepar, concussão, mossa, efervescência, agitação, terremoto, emoção, comoção, choque, estremeção, trepidação, tremor, impulso, balanço, alvoroço, secussão, perturbação]  \n",
       "3                                                                                                                                           [acampamento, aquartelamento, bivaque]  \n",
       "4                                                                                                               [resguardo, refúgio, abrigo, asilo, cobertura, reduto, valhacouto]  \n",
       "...                                                                                                                                                                            ...  \n",
       "210972                                                                                                                                                                     [úngue]  \n",
       "210973                                                                                                                                                              [uma, inédita]  \n",
       "210974                                                                       [sempar, singular, um, uno, incomparável, ímpar, só, inédito, inconfundível, sui generis, individual]  \n",
       "210975                                                                        [sós, uns, individuais, incomparáveis, incomparávéis, inconfundíveis, inéditos, ímpares, singulares]  \n",
       "210976                                                                                                                                                                      [útil]  \n",
       "\n",
       "[210977 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the dataframe\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kafkiano</td>\n",
       "      <td>[absurdo, confuso, surreal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaiser</td>\n",
       "      <td>[soberano, rei, majestade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kamikaze</td>\n",
       "      <td>[camicase, suicida]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kardecismo</td>\n",
       "      <td>[espiritismo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kit</td>\n",
       "      <td>[conjunto, coleção, estojo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kitsch</td>\n",
       "      <td>[ridículo, brega, cafona]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kiwi</td>\n",
       "      <td>[quivi, quiuí]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>know-how</td>\n",
       "      <td>[inaptidão, inexperiência]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word                     synonyms\n",
       "0    kafkiano  [absurdo, confuso, surreal]\n",
       "1      kaiser   [soberano, rei, majestade]\n",
       "2    kamikaze          [camicase, suicida]\n",
       "3  kardecismo                [espiritismo]\n",
       "4         kit  [conjunto, coleção, estojo]\n",
       "5      kitsch    [ridículo, brega, cafona]\n",
       "6        kiwi               [quivi, quiuí]\n",
       "7    know-how   [inaptidão, inexperiência]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the words and synonyms that starts with a specific letter\n",
    "letter = 'k'\n",
    "\n",
    "synonyms[synonyms['word'].str.lower().str.startswith(letter)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nações', 'distritos', 'países', 'pátrias', 'províncias']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the synonyms of a specific word\n",
    "word = 'regiões'\n",
    "\n",
    "list(synonyms[synonyms['word'] == word]['synonyms'].values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the augmentation techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonym replacement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possíveis melhorias:\n",
    "- Identificar o tipo de palavra (substantivo, adjetivo, verbo, etc) e aplicar a substituição apenas para palavras do mesmo tipo\n",
    "- Verificar o gênero da palavra e aplicar a substituição apenas para palavras do mesmo gênero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform the data augmentation replacing the words with their synonyms with the following conditions:\n",
    "# - Do not replace the words that are in the stop words list\n",
    "# - Replace a specific number of words (percentage of the total number of words in the text), if the number is not specified, replace 50% of the words)\n",
    "\n",
    "def synonyms_replacement(text, df, percentage=0.5):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "    \n",
    "    number_of_words = int(len(tokens) * percentage)\n",
    "    indexes = np.random.choice(len(tokens), number_of_words, replace=False)\n",
    "    \n",
    "    for index in indexes:\n",
    "        word = tokens[index]\n",
    "        \n",
    "        if word not in df['word'].values:\n",
    "            continue\n",
    "        \n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        synonyms = list(df[df['word'] == word]['synonyms'].values[0])\n",
    "        \n",
    "        if len(synonyms) == 0:\n",
    "            continue\n",
    "        \n",
    "        synonym_index = np.random.randint(0, len(synonyms))\n",
    "        tokens[index] = synonyms[synonym_index]\n",
    "        \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engenharia de dados é uma área que trata da transformação dos dados brutos de uma empresa\n",
      "Engenharia de dados é uma área que cuida da mudança dos saberes bravios de uma empresa\n"
     ]
    }
   ],
   "source": [
    "# Testing the synonyms replacement function\n",
    "sentence = 'Engenharia de dados é uma área que trata da transformação dos dados brutos de uma empresa' #Verificar problema de vírgula em frases\n",
    "\n",
    "augmented_sentence = synonyms_replacement(sentence, synonyms)\n",
    "\n",
    "# print the original text and the augmented text\n",
    "print(sentence)\n",
    "print(augmented_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A engenharia de dados é uma área que lida com a transformação dos dados brutos de uma empresa\n"
     ]
    }
   ],
   "source": [
    "# Translate the sentence to another language (English in this example) and then back to Portuguese\n",
    "first_translation = ts.translate_text(sentence, translator='google', to_language='es')\n",
    "second_translation = ts.translate_text(first_translation, translator='google', to_language='en')\n",
    "back_translation = ts.translate_text(second_translation, translator='google', to_language='pt')\n",
    "\n",
    "print(back_translation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tests (still in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('set.n.01'), Synset('set.n.02'), Synset('set.n.03'), Synset('stage_set.n.01'), Synset('set.n.05'), Synset('bent.n.01'), Synset('set.n.07'), Synset('set.n.08'), Synset('hardening.n.02'), Synset('set.n.10'), Synset('set.n.11'), Synset('set.n.12'), Synset('set.n.13'), Synset('put.v.01'), Synset('determine.v.03'), Synset('specify.v.02'), Synset('set.v.04'), Synset('set.v.05'), Synset('set.v.06'), Synset('fix.v.12'), Synset('set.v.08'), Synset('set.v.09'), Synset('set.v.10'), Synset('arrange.v.06'), Synset('plant.v.01'), Synset('set.v.13'), Synset('jell.v.01'), Synset('typeset.v.01'), Synset('set.v.16'), Synset('set.v.17'), Synset('set.v.18'), Synset('sic.v.01'), Synset('place.v.11'), Synset('rig.v.04'), Synset('set_up.v.04'), Synset('adjust.v.01'), Synset('fructify.v.03'), Synset('dress.v.16'), Synset('fit.s.02'), Synset('fixed.s.02'), Synset('located.s.01'), Synset('laid.s.01'), Synset('set.s.05'), Synset('determined.s.04'), Synset('hardened.s.05')]\n"
     ]
    }
   ],
   "source": [
    "# First, you're going to need to import wordnet:\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Then, we're going to use the term \"program\" to find synsets like so:\n",
    "syns = wordnet.synsets(\"set\")\n",
    "\n",
    "# Print all the synonyms\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "../scrapy-sinonimos/synonyms_scraper/synonyms.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# load the parquet file from ../scrapy-sinonimos/synonyms_scraper/synonyms_scraper/synonyms.parquet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mread_table(\u001b[39m'\u001b[39;49m\u001b[39m../scrapy-sinonimos/synonyms_scraper/synonyms.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mto_pandas()\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\artur\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2926\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2921\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2922\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2923\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2924\u001b[0m     )\n\u001b[0;32m   2925\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2926\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[0;32m   2927\u001b[0m         source,\n\u001b[0;32m   2928\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   2929\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   2930\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[0;32m   2931\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[0;32m   2932\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[0;32m   2933\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[0;32m   2934\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[0;32m   2935\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[0;32m   2936\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[0;32m   2937\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[0;32m   2938\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[0;32m   2939\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[0;32m   2940\u001b[0m     )\n\u001b[0;32m   2941\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m   2942\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[0;32m   2943\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[0;32m   2944\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\artur\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\site-packages\\pyarrow\\parquet\\core.py:2477\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[1;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m partitioning \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   2474\u001b[0m     partitioning \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mHivePartitioning\u001b[39m.\u001b[39mdiscover(\n\u001b[0;32m   2475\u001b[0m         infer_dictionary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 2477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mdataset(path_or_paths, filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[0;32m   2478\u001b[0m                            schema\u001b[39m=\u001b[39;49mschema, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mparquet_format,\n\u001b[0;32m   2479\u001b[0m                            partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[0;32m   2480\u001b[0m                            ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes)\n",
      "File \u001b[1;32mc:\\Users\\artur\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\site-packages\\pyarrow\\dataset.py:762\u001b[0m, in \u001b[0;36mdataset\u001b[1;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[0;32m    751\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m    752\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m    753\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    758\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mignore_prefixes\n\u001b[0;32m    759\u001b[0m )\n\u001b[0;32m    761\u001b[0m \u001b[39mif\u001b[39;00m _is_path_like(source):\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    763\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[0;32m    764\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[1;32mc:\\Users\\artur\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\site-packages\\pyarrow\\dataset.py:445\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[1;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[0;32m    443\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[0;32m    444\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 445\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[0;32m    447\u001b[0m options \u001b[39m=\u001b[39m FileSystemFactoryOptions(\n\u001b[0;32m    448\u001b[0m     partitioning\u001b[39m=\u001b[39mpartitioning,\n\u001b[0;32m    449\u001b[0m     partition_base_dir\u001b[39m=\u001b[39mpartition_base_dir,\n\u001b[0;32m    450\u001b[0m     exclude_invalid_files\u001b[39m=\u001b[39mexclude_invalid_files,\n\u001b[0;32m    451\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mselector_ignore_prefixes\n\u001b[0;32m    452\u001b[0m )\n\u001b[0;32m    453\u001b[0m factory \u001b[39m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[39mformat\u001b[39m, options)\n",
      "File \u001b[1;32mc:\\Users\\artur\\.pyenv\\pyenv-win\\versions\\3.11.1\\Lib\\site-packages\\pyarrow\\dataset.py:421\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[1;34m(path, filesystem)\u001b[0m\n\u001b[0;32m    419\u001b[0m     paths_or_selector \u001b[39m=\u001b[39m [path]\n\u001b[0;32m    420\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(path)\n\u001b[0;32m    423\u001b[0m \u001b[39mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ../scrapy-sinonimos/synonyms_scraper/synonyms.parquet"
     ]
    }
   ],
   "source": [
    "# load the parquet file from ../scrapy-sinonimos/synonyms_scraper/synonyms_scraper/synonyms.parquet\n",
    "df = pq.read_table('../scrapy-sinonimos/synonyms_scraper/synonyms.parquet').to_pandas()\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b00e70151b755bc02f6db254d67e0fdf553933eac08d0aa5854548684a01361"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
