{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "duuyE-GTYpPt"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JU_KAIcAYs_1"
      },
      "outputs": [],
      "source": [
        "vocabulary = []\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gCF_9CbUYt2T"
      },
      "outputs": [],
      "source": [
        "# Get all the words from the page and append them to the list \"words\"\n",
        "def get_words(page):\n",
        "    words_container = BeautifulSoup(page.content, 'html.parser').find(id=\"maintext\").find_all('p')[0]\n",
        "    \n",
        "    words = words_container.find_all('b')\n",
        "\n",
        "    # Extract the text from each word and clean it of unwanted characters and leading/trailing spaces\n",
        "    words = [word.get_text() for word in words]\n",
        "    words = [word.replace('â– ', '').replace('\\xa0', '') for word in words]\n",
        "    words = [word.strip() for word in words]\n",
        "\n",
        "    # Remove duplicates from the list\n",
        "    words = list(dict.fromkeys(words))\n",
        "    \n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLLrOxs5YuFE",
        "outputId": "bcc8c1ad-14e2-458f-9be6-1d2851753bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Letter: z Page: 1600\n"
          ]
        }
      ],
      "source": [
        "# Go through all the alphabet\n",
        "#for letter in alphabet:\n",
        "url = 'https://voc.cplp.org/index.php?action=simplesearch&query={letter}&sel=start&base=form&&start={page}'\n",
        "\n",
        "# Go through all the pages incrementing the page number by 100 each time until there are no more words to be found\n",
        "for letter in alphabet:\n",
        "    page = 0\n",
        "    while True:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Letter: \" + letter + \" Page: \" + str(page))\n",
        "        \n",
        "        page_url = url.format(letter=letter, page=page)\n",
        "        current_page = requests.get(page_url)\n",
        "        vocabulary += get_words(current_page)\n",
        "        \n",
        "        if len(get_words(current_page)) == 0:\n",
        "            break\n",
        "        \n",
        "        page += 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5emcK-dhYvT1"
      },
      "outputs": [],
      "source": [
        "# Save the vocabulary to a txt file\n",
        "with open('words.txt', 'w', encoding='utf-8') as f:\n",
        "    for word in vocabulary:\n",
        "        f.write(word + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "4b00e70151b755bc02f6db254d67e0fdf553933eac08d0aa5854548684a01361"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
